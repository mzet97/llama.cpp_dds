#pragma once

#include <cstdint>
#include <optional>
#include <string>
#include <vector>

namespace llama_dds {

/// A single turn in a conversation.
struct ChatMessage {
    std::string role;  ///< "system", "user", or "assistant"
    std::string content;
};

/// Inference request, mirrors the OpenAI Chat Completions API.
struct ChatCompletionRequest {
    std::string                             request_id;  ///< UUID generated by the client; used to correlate responses.
    std::string                             model;
    std::vector<ChatMessage>                messages;
    float                                   temperature = 0.7f;
    int32_t                                 max_tokens  = 256;
    bool                                    stream      = false;
    std::optional<float>                    top_p;
    std::optional<int32_t>                  n;
    std::optional<std::vector<std::string>> stop;
};

/// One response chunk (or the complete response for non-streaming requests).
struct ChatCompletionResponse {
    std::string                request_id;     ///< Matches the originating ChatCompletionRequest::request_id.
    std::string                model;
    std::string                content;        ///< Newly generated text for this chunk.
    std::optional<std::string> finish_reason;  ///< "stop", "length", or absent for intermediate chunks.
    bool                       is_final          = false;  ///< True on the last chunk; signals end-of-stream.
    int32_t                    prompt_tokens     = 0;
    int32_t                    completion_tokens = 0;
};

/// Periodic heartbeat published by the server so clients can detect availability.
struct ServerStatus {
    std::string server_id;
    int32_t     slots_idle       = 0;
    int32_t     slots_processing = 0;
    std::string model_loaded;
    bool        ready = false;
};

}  // namespace llama_dds
